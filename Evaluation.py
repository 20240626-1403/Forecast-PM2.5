# -*- coding: utf-8 -*-
"""Evaluation2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1znxtXrqCDOjE25xlEpWGbAE-5r6hZDuz
"""

# Install google drive
from google.colab import drive
drive.mount('/content/drive')

"""AC_GRU"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Model
from keras.layers import Input, Conv1D, MaxPooling1D, GRU, Dense, Dropout, Attention
from keras.callbacks import EarlyStopping
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from math import sqrt
from keras.layers import Concatenate

# Load and preprocess data
file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
datasets = pd.read_csv(file_path)

# Convert to float and normalize
values = datasets.values.astype('float32')
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(values)

# Create dataset for GRU
def create_dataset(data, time_lag=7):
    X, y = [], []
    for i in range(len(data) - time_lag):
        X.append(data[i:i + time_lag, 0])
        y.append(data[i + time_lag, 0])
    return np.array(X), np.array(y)

time_lag = 8
X, y = create_dataset(scaled_data, time_lag)

# Reshape input to be [samples, time steps, features]
X = X.reshape((X.shape[0], X.shape[1], 1))

# Split into training and test sets
train_size = int(len(X) * 0.8)
test_size = len(X) - train_size
train_X, test_X = X[:train_size], X[train_size:]
train_y, test_y = y[:train_size], y[train_size:]

# Define the input layer
input_layer = Input(shape=(time_lag, 1))

# Add the CNN layers
conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)
max_pool_layer = MaxPooling1D(pool_size=2)(conv_layer)

# Add the GRU layer
gru_layer = GRU(50, return_sequences=True)(max_pool_layer)

# Add the Attention layer
attention = Attention()([gru_layer, gru_layer])

# Concatenate the output of the Attention layer with the output of the GRU layer
concatenated = Concatenate()([attention, gru_layer])

# Another GRU layer
gru_layer2 = GRU(50)(concatenated)

# Dropout layer
dropout_layer = Dropout(0.2)(gru_layer2)

# Output layer
output_layer = Dense(1)(dropout_layer)

# Define the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='mae')

# Fit the model
early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_split=0.2,
                    verbose=2, shuffle=False, callbacks=[early_stop])

# Plot training history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.show()

# Make predictions
yhat = model.predict(test_X)

# Reshape test_X to match original dimensions for inverse scaling
test_X_reshaped = test_X.reshape((test_X.shape[0], test_X.shape[1] * test_X.shape[2]))

# Invert scaling for forecast
inv_yhat_placeholder = np.zeros((len(yhat), scaled_data.shape[1]))
inv_yhat_placeholder[:, 0] = yhat[:, 0]
inv_yhat = scaler.inverse_transform(inv_yhat_placeholder)
inv_yhat = inv_yhat[:,0]

# Invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y_placeholder = np.zeros((len(test_y), scaled_data.shape[1]))
inv_y_placeholder[:, 0] = test_y[:, 0]
inv_y = scaler.inverse_transform(inv_y_placeholder)
inv_y = inv_y[:,0]

# Calculate performance metrics
rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
mae = mean_absolute_error(inv_y, inv_yhat)
r2 = r2_score(inv_y, inv_yhat)
mape = np.mean(np.abs((inv_y - inv_yhat) / inv_y)) * 100

print(f'RMSE: {rmse:.2f}')
print(f'MAE: {mae:.2f}')
print(f'R-squared: {r2:.2f}')
print(f'MAPE: {mape:.2f}%')

# Plot observed vs predicted
plt.figure(figsize=(10,6))
plt.plot(inv_y, label='Observed PM2.5')
plt.plot(inv_yhat, label='Predicted PM2.5')
plt.xlabel('Time interval (hours)')
plt.ylabel('PM2.5 concentration')
plt.title('Observed vs Predicted PM2.5')
plt.legend()
plt.show()

"""Extra Tree"""

# extra tree(1, 4, 8, 12, 16, 20, 24)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)


def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}

for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = MultiOutputRegressor(ExtraTreesRegressor(random_state=42))

    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)

    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)


results_df = pd.DataFrame(results)


print(results_df)

"""LightGBM"""

# LightGBM (1, 4, 8, 12, 16, 20, 24)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np


file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:
    X = data[:-interval]
    y = data[interval:]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = MultiOutputRegressor(LGBMRegressor(random_state=42))
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)


results_df = pd.DataFrame(results)


print(results_df)

"""XGBoost"""

# XGBoost  (1, 4, 8, 12, 16, 20, 24)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]

results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}

for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model = MultiOutputRegressor(XGBRegressor(random_state=42))
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)
    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)

results_df = pd.DataFrame(results)
print(results_df)

"""Random forest"""

#RF (1, 4, 8, 12, 16, 20, 24)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)


def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    model = MultiOutputRegressor(RandomForestRegressor(random_state=42))
    model.fit(X_train, y_train)


    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)

results_df = pd.DataFrame(results)

print(results_df)

"""Gradient Boosting Regressor"""

# GB: Gradient Boosting Regressor (1, 4, 8, 12, 16, 20, 24)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    model = MultiOutputRegressor(GradientBoostingRegressor(random_state=42))
    model.fit(X_train, y_train)


    y_pred = model.predict(X_test)

    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)

results_df = pd.DataFrame(results)


print(results_df)

"""KNeighborsRegressor"""

# KNN (1, 4, 8, 12, 16, 20, 24)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

time_intervals = [1, 4, 8, 12, 16, 20, 24]

results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    model = MultiOutputRegressor(KNeighborsRegressor())
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)

    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)

results_df = pd.DataFrame(results)


print(results_df)

"""Ridge"""

# Ridge (1, 4, 8, 12, 16, 20, 24)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:
    X = data[:-interval]
    y = data[interval:]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    model = MultiOutputRegressor(Ridge(random_state=42))
    model.fit(X_train, y_train)


    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)

results_df = pd.DataFrame(results)


print(results_df)

"""Bayesian Ridge"""

# BR (1, 4, 8, 12, 16, 20, 24)


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import BayesianRidge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]

results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:
    X = data[:-interval]
    y = data[interval:]


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = MultiOutputRegressor(BayesianRidge())
    model.fit(X_train, y_train)


    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)


results_df = pd.DataFrame(results)

print(results_df)

"""Decision tree"""

# DT (1, 4, 8, 12, 16, 20, 24)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np


file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2


time_intervals = [1, 4, 8, 12, 16, 20, 24]

results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}


for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


    model = MultiOutputRegressor(DecisionTreeRegressor(random_state=42))
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)

    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)


results_df = pd.DataFrame(results)

print(results_df)

"""Determining window size for deep learning models"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import KFold
from math import sqrt

# Load the data
file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
datasets = pd.read_csv(file_path)

# Convert to float
values = datasets.values.astype('float32')

# Normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(values)

# Function to prepare dataset for LSTM
def create_dataset(data, time_lag=10):
    X, y = [], []
    for i in range(len(data) - time_lag):
        X.append(data[i:i + time_lag, 0])
        y.append(data[i + time_lag, 0])
    return np.array(X), np.array(y)

# Define the range of time lags to test (including specific time lags 3, 8, 10)
time_lags = [3, 8, 10] + list(range(1, 16))

# Remove duplicates and sort
time_lags = sorted(set(time_lags))

# K-Fold Cross-Validation
kf = KFold(n_splits=5)
mae_scores = []
rmse_scores = []

for time_lag in time_lags:
    X, y = create_dataset(scaled_data, time_lag)
    X = X.reshape((X.shape[0], X.shape[1], 1))

    mae_fold_scores = []
    rmse_fold_scores = []

    for train_index, test_index in kf.split(X):
        train_X, test_X = X[train_index], X[test_index]
        train_y, test_y = y[train_index], y[test_index]

        # Build the LSTM model
        model = Sequential()
        model.add(LSTM(50, return_sequences=True, input_shape=(time_lag, 1)))
        model.add(LSTM(50))
        model.add(Dropout(0.2))
        model.add(Dense(1))

        model.compile(optimizer='adam', loss='mae')

        # Fit the model
        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
        history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_split=0.2,
                            verbose=0, shuffle=False, callbacks=[early_stop])

        # Make predictions
        yhat = model.predict(test_X)

        # Invert scaling for forecast
        inv_yhat = scaler.inverse_transform(
            np.concatenate((yhat, np.zeros((yhat.shape[0], scaled_data.shape[1]-1))), axis=1)
        )[:, 0]

        # Invert scaling for actual
        test_y = test_y.reshape((len(test_y), 1))
        inv_y = scaler.inverse_transform(
            np.concatenate((test_y, np.zeros((test_y.shape[0], scaled_data.shape[1]-1))), axis=1)
        )[:, 0]

        # Calculate MAE
        mae = mean_absolute_error(inv_y, inv_yhat)
        mae_fold_scores.append(mae)

        # Calculate RMSE
        rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
        rmse_fold_scores.append(rmse)

    # Calculate average MAE and RMSE for this time lag
    mae_scores.append(np.mean(mae_fold_scores))
    rmse_scores.append(np.mean(rmse_fold_scores))

# Plot MAE and RMSE vs. Time Lag
plt.figure(figsize=(12, 6))
plt.plot(time_lags, mae_scores, marker='o', label='MAE')
plt.plot(time_lags, rmse_scores, marker='x', label='RMSE')
plt.title('MAE and RMSE vs. Time Lag')
plt.xlabel('Time Lag (hours)')
plt.ylabel('Error')
plt.legend()
plt.grid(True)
plt.show()

# Print best time lags
best_time_lag_mae = time_lags[np.argmin(mae_scores)]
best_time_lag_rmse = time_lags[np.argmin(rmse_scores)]

print(f"Best time lag based on MAE: {best_time_lag_mae} with MAE: {min(mae_scores)}")
print(f"Best time lag based on RMSE: {best_time_lag_rmse} with RMSE: {min(rmse_scores)}")

"""LSTM"""

# LSTM (1, 4, 8, 12, 16, 20, 24)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)


def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}

for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]

    time_steps = 8
    X_lstm, y_lstm = create_dataset(X, y, time_steps)

    X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)


 model = Sequential()
        model.add(LSTM(50, return_sequences=True, input_shape=(time_lag, 1)))
        model.add(LSTM(50))
        model.add(Dropout(0.2))
        model.add(Dense(1))

    model.compile(optimizer='adam', loss='mse')

        # Fit the model
        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
        history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_split=0.2,
                            verbose=0, shuffle=False, callbacks=[early_stop])





    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)

results_df = pd.DataFrame(results)

print(results_df)

"""GRU"""

# GRU (1, 4, 8, 12, 16, 20, 24)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense


file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
data = pd.read_csv(file_path)


def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)


time_intervals = [1, 4, 8, 12, 16, 20, 24]


results = {
    'Interval': [],
    'MSE': [],
    'RMSE': [],
    'MAE': [],
    'R2': []
}

for interval in time_intervals:

    X = data[:-interval]
    y = data[interval:]

    time_steps = 8
    X_gru, y_gru = create_dataset(X, y, time_steps)


    X_train, X_test, y_train, y_test = train_test_split(X_gru, y_gru, test_size=0.2, random_state=42)

model = Sequential()
        model.add(GRU(50, return_sequences=True, input_shape=(time_lag, 1)))
        model.add(GRU(50))
        model.add(Dropout(0.2))
        model.add(Dense(1))

    model.compile(optimizer='adam', loss='mse')

        # Fit the model
        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)
        history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_split=0.2,
                            verbose=0, shuffle=False, callbacks=[early_stop])



    y_pred = model.predict(X_test)


    mse, rmse, mae, r2 = calculate_metrics(y_test, y_pred)


    results['Interval'].append(interval)
    results['MSE'].append(mse)
    results['RMSE'].append(rmse)
    results['MAE'].append(mae)
    results['R2'].append(r2)


results_df = pd.DataFrame(results)


print(results_df)