# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w8N9bnJPvuDqb19vAMMGBiOQZwu__abK

Preprocessing
"""

from google.colab import drive
drive.mount('/content/drive')

"""Merging stations"""

import pandas as pd

aghdasiye = pd.read_excel("/content/aghdasiye.xlsx")#s1
golbarg = pd.read_excel("/content/golbarg.xlsx")#s2
masodiye = pd.read_excel("/content/masodiye.xlsx")#s3
pirozi = pd.read_excel("/content/pirozi.xlsx")#s4
ponak = pd.read_excel("/content/ponak.xlsx")#s5
setad_bohran = pd.read_excel("/content/setad_bohran.xlsx")#s6
shad_abad = pd.read_excel("/content/shad_abad.xlsx")#s7
shahrdari2 = pd.read_excel("/content/shahrdari2.xlsx")#s8
shahrdari21 = pd.read_excel("/content/shahrdari21.xlsx")#s9
shahre_rey = pd.read_excel("/content/shahre_rey.xlsx")#s10
sharif = pd.read_excel("/content/sharif.xlsx")#s11
tarbiyat_modares = pd.read_excel("/content/tarbiyat_modares.xlsx")#s12
file_names = [
    "golbarg.xlsx",
    "aghdasiye.xlsx",
    "masodiye.xlsx",
    "pirozi.xlsx",
    "ponak.xlsx",
    "setad_bohran.xlsx",
    "shad_abad.xlsx",
    "shahrdari2.xlsx",
    "shahrdari21.xlsx",
    "shahre_rey.xlsx",
    "sharif.xlsx",
    "tarbiyat_modares.xlsx"
]

merged_dataframe = pd.concat([pd.read_excel(f"/content/{file_name}") for file_name in file_names], ignore_index=True)
merged_dataframe.to_excel("/content/merged_data_updated.xlsx", index=False)
print(merged_dataframe.head())

"""outlier detection"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_excel("/content/merged_data_updated.xlsx")
grouped_df = df.groupby("station")
outlier_threshold = 1.5
outliers_list = []

for station, group in grouped_df:
    Q1 = group["PM 2.5 ug/m3"].quantile(0.25)
    Q3 = group["PM 2.5 ug/m3"].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - outlier_threshold * IQR
    upper_bound = Q3 + outlier_threshold * IQR
    outliers = group[(group["PM 2.5 ug/m3"] < lower_bound) | (group["PM 2.5 ug/m3"] > upper_bound)]
    outliers_list.extend(outliers["PM 2.5 ug/m3"].tolist())
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.boxplot(data=outliers_list)
plt.xlabel("Outliers")
plt.ylabel("PM 2.5 ug/m3")
plt.title("Box Plot of PM 2.5 ug/m3 Outliers")
plt.tight_layout()
plt.show()

"""box plot every station"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_excel("/content/merged_data_updated.xlsx")
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x="station", y="PM 2.5 ug/m3")
plt.xlabel("Station")
plt.ylabel("PM 2.5 ug/m3")
plt.title("Box Plot of PM 2.5 ug/m3 Outliers")
plt.savefig("outliers_boxplot.pdf", format="pdf")
plt.savefig("Noise_boxplot.jpg", format="jpg")
plt.tight_layout()
plt.show()

"""Statistical characteristics of the stations"""

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis
df = pd.read_excel("/content/merged_data_updated.xlsx")
station_stats = pd.DataFrame(columns=[
    "Station", "Min", "Max", "Mean", "Std Dev", "Missing Data Count",
    "Skewness", "Kurtosis", "Outliers Count", "Noise Count", "Noise Percentage", "Outliers Percentage"
])
grouped_df = df.groupby("station")
for station, group in grouped_df:
    total_rows = len(group)
    min_value = group["PM 2.5 ug/m3"].min()
    max_value = group["PM 2.5 ug/m3"].max()
    mean_value = group["PM 2.5 ug/m3"].mean()
    std_dev = group["PM 2.5 ug/m3"].std()
    missing_data_count = len(group) - group["PM 2.5 ug/m3"].count()
    skewness = skew(group["PM 2.5 ug/m3"])
    kurt = kurtosis(group["PM 2.5 ug/m3"])
        outliers_percentage = (outliers_count / total_rows) * 100
        station_stats = station_stats.append({
        "Station": station,
        "Min": min_value,
        "Max": max_value,
        "Mean": mean_value,
        "Std Dev": std_dev,
        "Missing Data Count": missing_data_count,
        "Skewness": skewness,
        "Kurtosis": kurt,
        "Outliers Count": outliers_count,
        "Noise Count": noise_count,
        "Noise Percentage": noise_percentage,
        "Outliers Percentage": outliers_percentage
    }, ignore_index=True)
station_stats.to_excel("station_statistics.xlsx", index=False)

"""xgboost"""

!pip install matplotlib xgboost pandas sklearn

import glob
import os
import matplotlib.pyplot as plt
import pandas as pd
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from xgboost import plot_importance

plt.style.use('fivethirtyeight')
CURRENT_PATH = os.path.abspath('')
DATA_PATH = os.path.join(CURRENT_PATH, "data")
RESULT_PATH = os.path.join(CURRENT_PATH, "XGBoost_result")
if not os.path.exists(RESULT_PATH):
    os.makedirs(RESULT_PATH)
FEATURES = ['Hour', 'O3 ppb', 'SO2 ppb', 'CO ppm', 'PM 10 ug/m3', 'NO2 ppb', 'Temperature',
            'Relative_Humidity', 'Total_Precipitation', 'Surface_Pressure']
LABEL = ['PM 2/5 ug/m3(EX)']
TRAIN_SIZE = .7
#Funcitons

Requirement already satisfied: matplotlib in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (3.5.3)
Requirement already satisfied: xgboost in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (1.6.2)
Requirement already satisfied: pandas in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (1.5.1)
Requirement already satisfied: sklearn in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (0.0)
Requirement already satisfied: python-dateutil>=2.7 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: fonttools>=4.22.0 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (4.38.0)
Requirement already satisfied: packaging>=20.0 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (21.3)
Requirement already satisfied: numpy>=1.17 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (1.23.4)
Requirement already satisfied: pillow>=6.2.0 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (9.2.0)
Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (1.4.4)
Requirement already satisfied: cycler>=0.10 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: pyparsing>=2.2.1 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from matplotlib) (3.0.9)
Requirement already satisfied: scipy in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from xgboost) (1.9.3)
Requirement already satisfied: pytz>=2020.1 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from pandas) (2022.5)
Requirement already satisfied: scikit-learn in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from sklearn) (1.1.2)
Requirement already satisfied: six>=1.5 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from scikit-learn->sklearn) (3.1.0)
Requirement already satisfied: joblib>=1.0.0 in c:\users\mostafa\documents\pyenv\py310\lib\site-packages (from scikit-learn->sklearn) (1.2.0)
def gregorian_to_jalali(gy, gm, gd):
    g_d_m = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]
    if gm > 2:
        gy2 = gy + 1
    else:
        gy2 = gy
    days = 355666 + (365 * gy) + ((gy2 + 3) // 4) - ((gy2 + 99) // 100) + ((gy2 + 399) // 400) + gd + g_d_m[gm - 1]
    jy = -1595 + (33 * (days // 12053))
    days %= 12053
    jy += 4 * (days // 1461)
    days %= 1461
    if days > 365:
        jy += (days - 1) // 365
        days = (days - 1) % 365
    if days < 186:
        jm = 1 + (days // 31)
        jd = 1 + (days % 31)
    else:
        jm = 7 + ((days - 186) // 30)
        jd = 1 + ((days - 186) % 30)
    return [jy, jm, jd]


def jalali_to_gregorian(jy, jm, jd):
    jy += 1595
    days = -355668 + (365 * jy) + ((jy // 33) * 8) + (((jy % 33) + 3) // 4) + jd
    if jm < 7:
        days += (jm - 1) * 31
    else:
        days += ((jm - 7) * 30) + 186
    gy = 400 * (days // 146097)
    days %= 146097
    if days > 36524:
        days -= 1
        gy += 100 * (days // 36524)
        days %= 36524
    if days >= 365:
        days += 1
    gy += 4 * (days // 1461)
    days %= 1461
    if days > 365:
        gy += ((days - 1) // 365)
        days = (days - 1) % 365
    gd = days + 1
    if (gy % 4 == 0 and gy % 100 != 0) or (gy % 400 == 0):
        kab = 29
    else:
        kab = 28
    sal_a = [0, 31, kab, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]
    gm = 0
    while gm < 13 and gd > sal_a[gm]:
        gd -= sal_a[gm]
        gm += 1
    return [gy, gm, gd]


def create_features(df, features, label=None):
    x = df[features]
    if label:
        y = df[label]
        return x, y
    return x


def train_xgboost(df, station):
    x, y = create_features(df, features=FEATURES, label=LABEL)
    #     scalerx = StandardScaler().fit(x)
    #     scalery = StandardScaler().fit(y)
    #     x = scalerx.transform(x)
    #     y = scalery.transform(y)
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=TRAIN_SIZE, shuffle=False)
    print(type(x_train))
    reg = xgb.XGBRegressor(n_estimators=1000)
    reg.fit(x_train, y_train,
            eval_set=[(x_train, y_train), (x_test, y_test)],
            early_stopping_rounds=50,
            verbose=False)  # Change verbose to True if you want to see it train

    save_path = os.path.join(RESULT_PATH, station)
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    importance_plot = plot_importance(reg, height=0.9, )
    importance_plot.figure.tight_layout()
    importance_plot.get_figure().savefig(os.path.join(save_path, f"{station}_importance.pdf"))
    y_test['PM2.5_Prediction'] = reg.predict(x_test)
    test_plot = y_test[['PM 2/5 ug/m3(EX)', 'PM2.5_Prediction']].plot(figsize=(35, 25), title='PM2.5')
    test_plot.get_figure().savefig(os.path.join(save_path, f"{station}_test.pdf"))
    with open(os.path.join(save_path, 'Errors.txt'), 'w') as file:
        file.write("mean_squared_error : {} \n".format(mean_squared_error(y_true=y_test['PM 2/5 ug/m3(EX)'],
                                                                          y_pred=y_test['PM2.5_Prediction'])))
        file.write("mean_absolute_error : {} \n".format(mean_absolute_error(y_true=y_test['PM 2/5 ug/m3(EX)'],
                                                                            y_pred=y_test['PM2.5_Prediction'])))
        file.write("r2_score : {} \n".format(r2_score(y_true=y_test['PM 2/5 ug/m3(EX)'],
                                                      y_pred=y_test['PM2.5_Prediction'])))

files = glob.glob(os.path.join(DATA_PATH, "*.xlsx"))
for file in files:
    df = pd.read_excel(file)
    df["Date_jalali"] = df["Date"]
    for idx in range(len(df)):
        y, m, d = jalali_to_gregorian(int(df.Date_jalali.iloc[idx].split("/")[0]),
                                      int(df.Date_jalali.iloc[idx].split("/")[1]),
                                      int(df.Date_jalali.iloc[idx].split("/")[2]), )
        df.Date.iloc[idx] = f"{y}-{m}-{d}"
    df.Date = pd.to_datetime(df.Date)
    station = os.path.split(file)[1].split(".")[0]
    print(station)
    train_xgboost(df, station=station)

"""Feature selection"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFECV
from sklearn.model_selection import KFold


dataset = pd.read_excel("/content/drive/MyDrive/Ph.d/pd.d/combined1_file.xlsx")
independent_vars = ['O3 ppb', 'CO ppm', 'NO2 ppb', 'NOx ppb', 'SO2 ppb', 'PM 10 ug/m3', 'wind_direction', 'wind speed', 'temperature', 'dewpoint_temperature', 'sea_level_pressure', 'station_pressure', 'relative_humidity']
X = dataset[independent_vars]
y = dataset['PM 2.5 ug/m3']
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)
linear_model = LinearRegression()
rfecv_linear = RFECV(estimator=linear_model, cv=KFold(n_splits=5), scoring='neg_mean_squared_error')
rfecv_linear.fit(X_normalized, y)
optimal_num_features_linear = rfecv_linear.n_features_
cross_val_scores_linear = -rfecv_linear.cv_results_['mean_test_score']
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Cross-validation score (RMSE)")
plt.plot(range(1, len(cross_val_scores_linear) + 1), cross_val_scores_linear, color='blue')
plt.scatter(optimal_num_features_linear, cross_val_scores_linear[optimal_num_features_linear - 1], color='red', marker='*')
selected_features = rfecv_linear.support_
best_features = [feature for feature, selected in zip(independent_vars, selected_features) if selected]
X_best = X_normalized[:, selected_features]
linear_model.fit(X_best, y)
feature_importance = linear_model.coef_
plt.figure()
plt.xlabel("Feature importance")
plt.ylabel("Feature name")
sorted_features = [feature for _, feature in sorted(zip(feature_importance, best_features), reverse=True)]
sorted_importance = sorted(feature_importance, reverse=True)
plt.barh(sorted_features, sorted_importance)
plt.show()

print("Optimal number of features for Linear Regression:", optimal_num_features_linear)

"""LOCF model performance for 10% missing values"""

import pandas as pd
import pandas as pd
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np
from numpy.random import choice

dataset = pd.read_excel("/content/drive/MyDrive/paper/pm2.5.xlsx")
dataset.shape
column_names = dataset.columns.tolist()
print(column_names)
file_path = "/content/drive/MyDrive/paper/pm2.5.xlsx"
dataset = pd.read_excel(file_path)
golbarg_data = dataset[dataset['station'] == 'golbarg']
golbarg_data['PM 2.5 ug/m3'] = golbarg_data['PM 2.5 ug/m3'].fillna(method='ffill', limit=None)
golbarg_data['PM 2.5 ug/m3'].fillna(initial_value, inplace=True)

dataset.loc[dataset['station'] == 'golbarg', 'PM 2.5 ug/m3'] = golbarg_data['PM 2.5 ug/m3']
print(dataset)
for station_name in dataset['station'].unique():
    station_data = dataset[dataset['station'] == station_name]
    station_data['PM 2.5 ug/m3'] = station_data['PM 2.5 ug/m3'].fillna(method='ffill', limit=None)
    station_data['PM 2.5 ug/m3'].fillna(initial_value, inplace=True)
    dataset.loc[dataset['station'] == station_name, 'PM 2.5 ug/m3'] = station_data['PM 2.5 ug/m3']

imputed_file_path = "/content/drive/MyDrive/paper/pm2.5_imputed.xlsx"
dataset.to_excel(imputed_file_path, index=False)

print("imputed data with missing values ​​for all stations are saved in the new file.")
#LOCF (evaluatin 10%,15%,20%)

if dataset.isnull().values.any():
    print("Data contains NaN values")
else:
    print("Data does not contain NaN values")

original_data_path = "/content/drive/MyDrive/paper/pm2.5_imputed_LOCF.xlsx"
original_data = pd.read_excel(original_data_path)

original_data_copy = original_data.copy()

missing_indices = choice(original_data.index, size=int(0.1 * len(original_data)), replace=False)
original_data.loc[missing_indices, 'PM 2.5 ug/m3'] = np.nan

original_data['PM 2.5 ug/m3'] = original_data['PM 2.5 ug/m3'].fillna(method='ffill')

lof_imputed_path = "/content/drive/MyDrive/paper/pm2.5_imputed_LOCF_10percent.xlsx"
original_data.to_excel(lof_imputed_path, index=False)
def evaluate_imputation_performance(original_data, imputed_data):
    r2 = r2_score(original_data, imputed_data)
    rmse = mean_squared_error(original_data, imputed_data, squared=False)
    mae = mean_absolute_error(original_data, imputed_data)
    return r2, rmse, mae
r2_lof, rmse_lof, mae_lof = evaluate_imputation_performance(original_data_copy['PM 2.5 ug/m3'], original_data['PM 2.5 ug/m3'])
print("LOCF model performance for 10% missing values:")
print("R^2:", r2_lof)
print("RMSE:", rmse_lof)
print("MAE:", mae_lof)

"""Multiple Imputation by Chained Equations (MICE)"""

import pandas as pd
from fancyimpute import IterativeImputer
import pandas as pd
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from fancyimpute import IterativeImputer
import numpy as np
from numpy.random import choice

file_path = "/content/drive/MyDrive/paper/pm2.5.xlsx"
dataset = pd.read_excel(file_path)
imputer = IterativeImputer()
dataset_imputed = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)
print(dataset_imputed)
imputer = IterativeImputer()
missing_indices = choice(dataset.index, size=int(0.1 * len(dataset)), replace=False)
dataset.loc[missing_indices, 'PM 2.5 ug/m3'] = np.nan
dataset_imputed = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)
def evaluate_imputation_performance(original_data, imputed_data):
    r2 = r2_score(original_data, imputed_data)
    rmse = mean_squared_error(original_data, imputed_data, squared=False)
    mae = mean_absolute_error(original_data, imputed_data)
    return r2, rmse, mae

r2_mice, rmse_mice, mae_mice = evaluate_imputation_performance(dataset['PM 2.5 ug/m3'], dataset_imputed['PM 2.5 ug/m3'])
print("LOCF model performance for 10% missing values")
print("R^2:", r2_mice)
print("RMSE:", rmse_mice)
print("MAE:", mae_mice)

"""spatial and temporal corelation"""

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import numpy as np
import pandas as pd
import seaborn as sns


file_path = '/content/drive/MyDrive/rahbord/combined_pm2.csv'
df = pd.read_csv(file_path)

#temporal corelation
time_lags_zoomed_out = range(0, 101)
time_lags_zoomed_in = range(0, 25, 4)
temporal_corr_zoomed_out = {}
temporal_corr_zoomed_in = {}

for station in df.columns:
    temporal_corr_zoomed_out[station] = [df[station].corr(df[station].shift(-lag)) for lag in time_lags_zoomed_out]
    temporal_corr_zoomed_in[station] = [df[station].corr(df[station].shift(-lag)) for lag in time_lags_zoomed_in]


fig = plt.figure(figsize=(8, 8))
gs = fig.add_gridspec(3, 4)
ax_main = fig.add_subplot(gs[1:3, :])


for station in df.columns:
    ax_main.plot(time_lags_zoomed_out, temporal_corr_zoomed_out[station], label=station, linewidth=1)


for station in df.columns:
    ax_main.plot(time_lags_zoomed_in, temporal_corr_zoomed_in[station], linewidth=1, marker='o', markersize=4)

ax_main.set_xlabel('Time Lag (hours)')
ax_main.set_ylabel('Correlation Coefficient')
ax_main.grid(True)
ax_main.set_xlim(0, 100)


legend_labels = [f'S{i+1}' for i in range(len(df.columns))]
ax_legend = fig.add_subplot(gs[0, :])
ax_legend.axis('off')
ax_legend.legend(legend_labels, loc='center', ncol=1)


from mpl_toolkits.axes_grid1.inset_locator import inset_axes
ax_inset = inset_axes(ax_main, width="30%", height="30%", loc='upper right')
for station in df.columns:
    ax_inset.plot(time_lags_zoomed_in, temporal_corr_zoomed_in[station], linewidth=1, marker='o', markersize=4)
ax_inset.set_title('Zoomed In')
ax_inset.set_xlabel('Time Lag (hours)')
ax_inset.set_ylabel('Correlation Coefficient')
ax_inset.grid(True)
ax_inset.set_xlim(0, 24)

plt.tight_layout()
plt.show()

spatial_corr = df.corr()


plt.figure(figsize=(10, 8))
sns.heatmap(spatial_corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Spatial Correlation of PM2.5 Concentration')
plt.show()